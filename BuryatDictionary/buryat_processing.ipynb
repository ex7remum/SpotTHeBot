{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanfordnlp.download('bxr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanfordnlp.Pipeline(lang='bxr', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#предобаботка текста\n",
    "alphabet = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоӨөПпРрСсТтУуҮүФфХхҺһЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
    "special = \",.;:!@#$%^&*(){}[]\\\"\\xa0\"\n",
    "def parse(word, output_file):\n",
    "    if len(word) == 0:\n",
    "        return\n",
    "    if word[0] in alphabet:\n",
    "        pos = alphabet.find(word[0])\n",
    "        if pos % 2 == 0:\n",
    "            word = alphabet[pos + 1] + word[1:]\n",
    "        for symbol in special:\n",
    "            word = word.replace(symbol, '')\n",
    "        index = 0\n",
    "        for letter in word:\n",
    "            pos_letter = alphabet.find(letter)\n",
    "            if pos_letter % 2 == 0:\n",
    "                word = word[:index] + alphabet[pos_letter + 1] + word[index + 1 :]\n",
    "            index += 1\n",
    "        if len(word) >= 2:\n",
    "            word_nlp = nlp(word)\n",
    "            for sent in word_nlp.sentences:\n",
    "                    for w in sent.words:\n",
    "                        lemma = w.lemma\n",
    "                        output_file.write(lemma)\n",
    "                        output_file.write(' ')\n",
    "\n",
    "#получение корпуса\n",
    "def make_corpus(input_paths, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding=\"utf_8_sig\") as output_file:\n",
    "        cnt = 0\n",
    "        for input_path in input_paths:\n",
    "            with open(input_path, encoding = 'utf_8') as data:\n",
    "                data = data.read().split(\"</doc>\")\n",
    "                for i in range(len(data)):\n",
    "                    cnt_words = len(data[i].split(' '))\n",
    "                    if cnt_words >= 20:\n",
    "                        cnt += 1\n",
    "                        cur_text = data[i][(data[i].find('>') + 1):]\n",
    "                        words = cur_text.replace(\"\\n\", \" \").split(\" \")\n",
    "                        for word in words:\n",
    "                            cur = parse(word, output_file)\n",
    "                        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_corpus(['Corpus/Raw_data/wiki_00','Corpus/Raw_data/wiki_01','Corpus/Raw_data/wiki_02',\n",
    "             'Corpus/Raw_data/wiki_03','Corpus/Raw_data/wiki_04', 'Corpus/Raw_data/wiki_05'], 'Corpus/processed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def make_table_and_dict(corpus_path, min_df, max_df, token_pattern = None, use_idf = True):\n",
    "    with open(corpus_path, 'r', encoding=\"utf8\") as corpus_file:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
    "    return data_vectorized, vectorizer.get_feature_names(), vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buryat_data_vectorized, buryat_dictionary, idfs = make_table_and_dict('./Corpus/processed.txt', 3, 0.8)\n",
    "pairs = dict(zip(buryat_dictionary, idfs))\n",
    "with open('./Corpus/cutted_buryat_dict.txt', 'w', encoding=\"utf8\") as output_file:\n",
    "    for word in buryat_dictionary:\n",
    "        output_file.write(word)\n",
    "        output_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Corpus/' + 'Tf-Idf_Matrix.npy', 'wb') as f:\n",
    "    np.save(f, buryat_data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получение SVD-разложения\n",
    "def create_table(data_vectorized, k, name):\n",
    "    u, sigma, vt = svds(data_vectorized, k)\n",
    "    with open('./Corpus/SVD_U.npy', 'wb') as f:\n",
    "        np.save(f, u)\n",
    "    with open('./Corpus/SVD_sigma.npy', 'wb') as f:\n",
    "        np.save(f, sigma)\n",
    "    print(sigma)\n",
    "    with open('./Corpus/SVD_VT.npy', 'wb') as f:\n",
    "        np.save(f, vt)\n",
    "    with open('./Corpus/' + name + str(k) + '.npy', 'wb') as f:\n",
    "        np.save(f, np.dot(np.diag(sigma), vt).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(buryat_data_vectorized, 1024, 'buryat_sigma_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = np.load('./Corpus/buryat_sigma_v1024.npy')\n",
    "i = 0\n",
    "dictionary = {}\n",
    "with open('./Corpus/cutted_buryat_dict.txt',  'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        dictionary[line[:-1]] = table[i] #removing '\\n' at the end of the words\n",
    "        i += 1\n",
    "with open('./Corpus/buryat_dictionary.npy', 'wb') as f:\n",
    "        np.save(f, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получаем датасет с заданными параметрами N - длина N-грам и M - размерность слова\n",
    "N = 3\n",
    "M = 10\n",
    "new_dict = np.load('./Corpus/buryat_dictionary.npy', allow_pickle='TRUE')\n",
    "dictionary = new_dict.item()\n",
    "cnt = 0\n",
    "ngrams = []\n",
    "\n",
    "with open('./Corpus/processed.txt',  'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        current = line.split(' ')\n",
    "        for i in range(len(current) - N + 1):\n",
    "            if current[i] in dictionary and current[i + 1] in dictionary and current[i + 2] in dictionary:\n",
    "                result = [*dictionary[current[i]][:M], *dictionary[current[i + 1]][:M], *dictionary[current[i + 2]][:M]]\n",
    "                ngrams.append(result)\n",
    "res = np.array(ngrams)\n",
    "print(res.shape)\n",
    "with open('./Corpus/result_dataset.npy', 'wb') as f:\n",
    "    np.save(f, ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
